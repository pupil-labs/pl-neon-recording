{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pupil Labs Neon Recording","text":"<p>Functionality for loading Neon recordings in native recording format</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pupil-labs-neon-recording\n</code></pre> <p>or</p> <pre><code>pip install -e git+https://github.com/pupil-labs/pl-neon-recording.git\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available at https://pupil-labs.github.io/pl-neon-recording/</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import sys\n\nimport pupil_labs.neon_recording as nr\n\nif len(sys.argv) &lt; 2:\n    print(\"Usage:\")\n    print(\"python basic_usage.py path/to/recording/folder\")\n\n# Open a recording\nrecording = nr.open(sys.argv[1])\n\n# get basic info\nprint(\"Recording Info:\")\nprint(f\"\\tStart time (ns): {recording.start_ts}\")\nprint(f\"\\tWearer         : {recording.wearer['name']}\")\nprint(f\"\\tDevice serial  : {recording.device_serial}\")\nprint(f\"\\tGaze samples   : {len(recording.gaze)}\")\nprint(\"\")\n\n# read 10 gaze samples\nprint(\"First 10 gaze samples:\")\ntimestamps = recording.gaze.ts[:10]\nsubsample = recording.gaze.sample(timestamps)\nfor gaze_datum in subsample:\n    print(f\"\\t{gaze_datum.ts} : ({gaze_datum.x:0.2f}, {gaze_datum.y:0.2f})\")\n</code></pre>"},{"location":"#blinks-and-fixations","title":"Blinks And Fixations","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nimport pupil_labs.neon_recording as nr  # noqa: E402\n\n\nclass EventTracker:\n    def __init__(self, stream):\n        self.idx = 0\n        self.itr = iter(stream)\n        self.next_event = next(self.itr)\n\n    def in_event(self, ts):\n        if self.next_event is None:\n            return False\n\n        return (\n            self.next_event.start_timestamp_ns &lt; ts &lt; self.next_event.end_timestamp_ns\n        )\n\n    def step_to(self, ts):\n        try:\n            while self.next_event is not None and self.next_event.end_timestamp_ns &lt; ts:\n                self.next_event = next(self.itr)\n                self.idx += 1\n\n        except StopIteration:\n            self.next_event = None\n\n        return self.in_event(ts)\n\n\ndef write_text(image, text, x, y):\n    return cv2.putText(\n        image,\n        text,\n        (x, y),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        1,\n        (0, 0, 255),\n        2,\n        cv2.LINE_AA,\n    )\n\n\ndef make_overlaid_video(recording_dir, output_video_path, fps=30):\n    recording = nr.open(recording_dir)\n\n    video_writer = cv2.VideoWriter(\n        str(output_video_path),\n        cv2.VideoWriter_fourcc(*\"MJPG\"),\n        fps,\n        (recording.eye.width, recording.eye.height),\n    )\n\n    output_timestamps = np.arange(\n        recording.eye.ts[0], recording.eye.ts[-1], int(1e9 / fps)\n    )\n    eye_frames = recording.eye.sample(output_timestamps)\n\n    fixations_only = recording.fixations[recording.fixations[\"event_type\"] == 1]\n    event_trackers = {\n        \"Fixation\": EventTracker(fixations_only),\n        \"Blink\": EventTracker(recording.blinks),\n    }\n\n    for frame in tqdm(eye_frames):\n        frame_pixels = frame.bgr\n\n        text_y = 0\n        for stream, tracker in event_trackers.items():\n            text_y += 40\n            if tracker.step_to(frame.ts):\n                frame_pixels = write_text(\n                    frame_pixels, f\"{stream} {tracker.idx + 1}\", 0, text_y\n                )\n\n        video_writer.write(frame_pixels)\n        cv2.imshow(\"Frame\", frame_pixels)\n        cv2.pollKey()\n\n    video_writer.release()\n\n\nif __name__ == \"__main__\":\n    make_overlaid_video(sys.argv[1], \"blinks-and-fixations.mp4\")\n</code></pre>"},{"location":"#data-access","title":"Data Access","text":"<pre><code>import sys\nfrom collections.abc import Mapping\nfrom datetime import datetime\n\nimport numpy as np\n\nimport pupil_labs.neon_recording as nr\nfrom pupil_labs.neon_recording.stream.stream import Stream\n\nif len(sys.argv) &lt; 2:\n    print(\"Usage:\")\n    print(\"python basic_usage.py path/to/recording/folder\")\n\n# Open a recording\nrecording = nr.open(sys.argv[1])\n\n\ndef pretty_format(mapping: Mapping):\n    output = []\n    pad = \"    \"\n    keys = mapping.keys()\n    n = max(len(key) for key in keys)\n    for k, v in mapping.items():\n        v_repr_lines = str(v).splitlines()\n        output.append(f\"{pad}{k:&gt;{n}}: {v_repr_lines[0]}\")\n        if len(v_repr_lines) &gt; 1:\n            output.extend(f\"{pad + '  '}{n * ' '}{line}\" for line in v_repr_lines[1:])\n    return \"\\n\".join(output)\n\n\nprint(\"Basic Recording Info:\")\nprint_data = {\n    \"Recording ID\": recording.id,\n    \"Start time (ns since unix epoch)\": f\"{recording.start_ts}\",\n    \"Start time (datetime)\": f\"{datetime.fromtimestamp(recording.start_ts / 1e9)}\",\n    \"Duration (nanoseconds)\": f\"{recording.duration}\",\n    \"Duration (seconds)\": f\"{recording.duration / 1e9}\",\n    \"Wearer\": f\"{recording.wearer['name']} ({recording.wearer['uuid']})\",\n    \"Device serial\": recording.device_serial,\n    \"App version\": recording.info[\"app_version\"],\n    \"Data format\": recording.info[\"data_format_version\"],\n    \"Gaze Offset\": recording.info[\"gaze_offset\"],\n}\nprint(pretty_format(print_data))\n\nstreams: list[Stream] = [\n    recording.gaze,\n    recording.imu,\n    recording.eye_state,\n    recording.blinks,\n    recording.fixations,\n    recording.worn,\n    recording.eye,\n    recording.scene,\n    recording.audio,\n]\nprint()\nprint(\"Recording streams:\")\nprint(\n    pretty_format({\n        f\"{stream.name} ({len(stream)} samples)\": \"\\n\" + pretty_format(stream[0])\n        for stream in streams\n    })\n)\n\n# Data can be converted to numpy or dataframes, however multi column properties\n# like .xy will not be included\n\nprint()\nprint(\"Gaze data as numpy array:\")\ngaze_np = recording.gaze.data\nprint()\nprint(gaze_np)\n\n\nprint()\nprint(\"Gaze data as pandas dataframe:\")\ngaze_df = recording.gaze.pd\nprint()\nprint(gaze_df)\n\n\nprint()\nprint(\"Getting data from a stream:\")\n\nprint()\nprint(\"Gaze data\", recording.gaze)\n# GazeArray([(1741948698620648018, 966.3677 , 439.58817),\n#            (1741948698630654018, 965.9669 , 441.60403),\n#            (1741948698635648018, 964.2665 , 442.4974 ), ...,\n#            (1741948717448190018, 757.85815, 852.34644),\n#            (1741948717453190018, 766.53174, 857.3709 ),\n#            (1741948717458190018, 730.93604, 851.53723)],\n#           dtype=[('ts', '&lt;i8'), ('x', '&lt;f4'), ('y', '&lt;f4')])\n\nprint()\nprint(\"Gaze ts via prop\", recording.gaze.ts)\nprint(\"Gaze ts via key\", recording.gaze[\"ts\"])\n# array([1741948698620648018, 1741948698630654018, 1741948698635648018, ...,\n#        1741948717448190018, 1741948717453190018, 1741948717458190018])\n\nprint()\nprint(\"Gaze x coords via prop\", recording.gaze.x)\nprint(\"Gaze x coords via key\", recording.gaze[\"x\"])\n# array([966.3677 , 965.9669 , 964.2665 , ..., 757.85815, 766.53174,\n#        730.93604], dtype=float32)\n\nprint()\nprint(\"Gaze y coords via prop\", recording.gaze.y)\nprint(\"Gaze y coords via key\", recording.gaze[\"y\"])\n# array([439.58817, 441.60403, 442.4974 , ..., 852.34644, 857.3709 ,\n#        851.53723], dtype=float32)\n\nprint()\nprint(\"Gaze xy coords\", recording.gaze.xy)\nprint(\"Gaze xy coords\", recording.gaze[[\"x\", \"y\"]])\n# array([[966.3677 , 439.58817],\n#        ...,\n#        [730.93604, 851.53723]], dtype=float32)\n\nprint()\nprint(\"Gaze ts and x and y\", recording.gaze[[\"ts\", \"x\", \"y\"]])\n# array([[1.74194870e+18, 9.66367676e+02, 4.39588165e+02],\n#        ...,\n#        [1.74194872e+18, 7.30936035e+02, 8.51537231e+02]])\n\nprint()\nprint(\"Sampling data:\")\n\nprint()\nprint(\"Get closest gaze for scene frames\")\nclosest_gaze_to_scene = recording.gaze.sample(recording.scene.ts)\nprint(closest_gaze_to_scene)\nprint(\n    \"closest_gaze_to_scene_times\",\n    (closest_gaze_to_scene.ts - recording.start_ts) / 1e9,\n)\n\n\nprint()\nprint(\"Get closest before gaze for scene frames\")\nclosest_gaze_before_scene = recording.gaze.sample(recording.scene.ts, method=\"before\")\nprint(closest_gaze_before_scene)\nprint(\n    \"closest_gaze_before_scene_times\",\n    (closest_gaze_before_scene.ts - recording.start_ts) / 1e9,\n)\n\n\nprint()\nprint(\"Sampled data can be resampled\")\n\nprint()\nprint(\"Closest gaze sampled at 1 fps\")\nclosest_gaze_to_scene_at_one_fps = closest_gaze_before_scene.sample(\n    np.arange(closest_gaze_to_scene.ts[0], closest_gaze_to_scene.ts[-1], 1e9 / 1)\n)\nprint(closest_gaze_to_scene_at_one_fps)\nprint(\n    \"closest_gaze_to_scene_at_one_fps_times\",\n    (closest_gaze_to_scene_at_one_fps.ts - recording.start_ts) / 1e9,\n)\n</code></pre>"},{"location":"#eye-overlay","title":"Eye Overlay","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nimport pupil_labs.neon_recording as nr  # noqa: E402, I001\nfrom pupil_labs.neon_recording.stream.av_stream.video_stream import GrayFrame  # noqa: E402\n\n\ndef overlay_image(img, img_overlay, x, y):\n    \"\"\"Overlay `img_overlay` onto `img` at (x, y).\"\"\"\n    # Image ranges\n    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n\n    # Overlay ranges\n    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n\n    if y1 &gt;= y2 or x1 &gt;= x2 or y1o &gt;= y2o or x1o &gt;= x2o:\n        return\n\n    img_crop = img[y1:y2, x1:x2]\n    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n    img_crop[:] = img_overlay_crop\n\n\ndef make_overlaid_video(recording_dir, output_video_path, fps=None):\n    recording = nr.open(recording_dir)\n\n    video_writer = cv2.VideoWriter(\n        str(output_video_path),\n        cv2.VideoWriter_fourcc(*\"MJPG\"),\n        fps,\n        (recording.scene.width, recording.scene.height),\n    )\n\n    if fps is None:\n        output_timestamps = recording.scene.ts\n        fps = 30\n    else:\n        output_timestamps = np.arange(\n            recording.scene.ts[0], recording.scene.ts[-1], int(1e9 / fps)\n        )\n\n    combined_data = zip(\n        output_timestamps,\n        recording.scene.sample(output_timestamps),\n        recording.eye.sample(output_timestamps),\n    )\n\n    frame_idx = 0\n    for ts, scene_frame, eye_frame in tqdm(combined_data, total=len(output_timestamps)):\n        frame_idx += 1  # noqa: SIM113\n\n        if abs(scene_frame.ts - ts) &lt; 2e9 / fps:\n            # if the video frame timestamp is too far ahead or behind temporally,\n            # replace it with a gray frame\n            frame_pixels = scene_frame.bgr\n        else:\n            frame_pixels = GrayFrame(scene_frame.width, scene_frame.height).bgr\n\n        if abs(eye_frame.ts - ts) &lt; 2e9 / fps:\n            # if the video frame timestamp is too far ahead or behind temporally,\n            # replace it with a gray frame\n            eye_pixels = cv2.cvtColor(eye_frame.gray, cv2.COLOR_GRAY2BGR)\n        else:\n            eye_pixels = GrayFrame(eye_frame.width, eye_frame.height).bgr\n\n        overlay_image(frame_pixels, eye_pixels, 50, 50)\n\n        video_writer.write(frame_pixels)\n        cv2.imshow(\"Frame\", frame_pixels)\n        cv2.pollKey()\n\n    video_writer.release()\n\n\nif __name__ == \"__main__\":\n    make_overlaid_video(sys.argv[1], \"eye-overlay-output-video.avi\", None)\n</code></pre>"},{"location":"#eye-state","title":"Eye State","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nimport pupil_labs.neon_recording as nr  # noqa: E402, I001\nfrom pupil_labs.neon_recording.stream.av_stream.video_stream import GrayFrame  # noqa: E402\n\n\ndef overlay_image(img, img_overlay, x, y):\n    \"\"\"Overlay `img_overlay` onto `img` at (x, y).\"\"\"\n    # Image ranges\n    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n\n    # Overlay ranges\n    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n\n    if y1 &gt;= y2 or x1 &gt;= x2 or y1o &gt;= y2o or x1o &gt;= x2o:\n        return\n\n    img_crop = img[y1:y2, x1:x2]\n    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n    img_crop[:] = img_overlay_crop\n\n\ndef plot(img, data, value_range, x_width, color, line_width=2):\n    for idx in range(1, len(data)):\n        x_values = [int(idx2 * x_width) for idx2 in [idx - 1, idx]]\n\n        y_norms = [\n            (data[idx2] - value_range[0]) / (value_range[1] - value_range[0])\n            for idx2 in [idx - 1, idx]\n        ]\n        y_values = [int(y_norm * img.shape[0]) for y_norm in y_norms]\n\n        points = [[*v] for v in zip(x_values, y_values)]\n\n        cv2.line(img, points[0], points[1], color, line_width)\n\n\ndef make_eye_state_video(recording_dir, output_video_path):\n    recording = nr.open(recording_dir)\n\n    fps = 200\n\n    video_writer = cv2.VideoWriter(\n        str(output_video_path),\n        cv2.VideoWriter_fourcc(*\"MJPG\"),\n        fps,\n        (recording.eye.width, recording.eye.height),\n    )\n\n    output_timestamps = np.arange(\n        recording.eye.ts[0], recording.eye.ts[-1], int(1e9 / fps)\n    )\n\n    eye_video_sampled = recording.eye.sample(output_timestamps)\n    eye_state_sampled = recording.eye_state.sample(output_timestamps)\n    combined_data = zip(\n        output_timestamps,\n        eye_video_sampled,\n        eye_state_sampled,\n    )\n\n    plot_metas = {\n        \"optical_axis_left_x\": {\"color\": [0, 0, 255]},\n        \"optical_axis_left_y\": {\n            \"color\": [0, 255, 0],\n        },\n        \"optical_axis_left_z\": {\n            \"color\": [255, 0, 0],\n        },\n    }\n\n    for plot_name, plot_meta in plot_metas.items():\n        plot_meta[\"range\"] = (\n            np.min(recording.eye_state.data[plot_name]),\n            np.max(recording.eye_state.data[plot_name]),\n        )\n\n    plot_duration_secs = 0.5\n    plot_point_count = plot_duration_secs * fps\n    plot_x_width = recording.eye.width / plot_point_count\n\n    for ts, eye_frame, _eye_state in tqdm(combined_data, total=len(output_timestamps)):\n        if abs(eye_frame.ts - ts) &lt; 2e9 / fps:\n            eye_pixels = cv2.cvtColor(eye_frame.gray, cv2.COLOR_GRAY2BGR)\n        else:\n            eye_pixels = GrayFrame(eye_frame.width, eye_frame.height).bgr\n\n        for plot_name, plot_meta in plot_metas.items():\n            min_ts = ts - plot_duration_secs * 1e9\n            time_frame = (min_ts &lt; eye_state_sampled.ts) &amp; (eye_state_sampled.ts &lt;= ts)\n            plot_data = eye_state_sampled[time_frame][plot_name]\n            plot(\n                eye_pixels,\n                plot_data,\n                plot_meta[\"range\"],\n                plot_x_width,\n                plot_meta[\"color\"],\n            )\n\n        video_writer.write(eye_pixels)\n        cv2.imshow(\"Frame\", eye_pixels)\n        cv2.pollKey()\n\n    video_writer.release()\n\n\nif __name__ == \"__main__\":\n    make_eye_state_video(sys.argv[1], \"eye-state-output-video.avi\")\n</code></pre>"},{"location":"#find-clap","title":"Find Clap","text":"<pre><code>import sys\n\nimport numpy as np\n\nimport pupil_labs.neon_recording as nr\n\n\ndef find_clap(recording_dir):\n    recording = nr.open(recording_dir)\n\n    max_rms = -1\n    max_time = -1\n\n    for frame in recording.audio:\n        audio_data = frame.to_ndarray()\n        rms = np.sqrt(np.mean(audio_data**2))\n\n        if rms &gt; max_rms:\n            max_rms = rms\n            max_time = frame.ts\n\n    rel_time = (max_time - recording.start_ts) / 1e9\n    print(f\"The loudest audio occurs at {max_time} (rel={rel_time}), rms = {max_rms}.\")\n\n\nif __name__ == \"__main__\":\n    find_clap(sys.argv[1])\n</code></pre>"},{"location":"#gaze-overlay","title":"Gaze Overlay","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nimport pupil_labs.neon_recording as nr  # noqa: E402, I001\nfrom pupil_labs.neon_recording.stream.av_stream.video_stream import GrayFrame  # noqa: E402\n\n\ndef make_overlaid_video(recording_dir, output_video_path, fps=None):\n    recording = nr.open(recording_dir)\n\n    video_writer = cv2.VideoWriter(\n        str(output_video_path),\n        cv2.VideoWriter_fourcc(*\"MJPG\"),\n        fps,\n        (recording.scene.width, recording.scene.height),\n    )\n\n    if fps is None:\n        output_timestamps = recording.scene.ts\n        fps = 30\n    else:\n        output_timestamps = np.arange(\n            recording.scene.ts[0], recording.scene.ts[-1], 1e9 / fps\n        )\n\n    scene_datas = recording.scene.sample(output_timestamps)\n    combined_data = zip(\n        output_timestamps,\n        scene_datas,\n        recording.gaze.sample(output_timestamps),\n    )\n\n    for ts, scene_frame, gaze_datum in tqdm(\n        combined_data, total=len(output_timestamps)\n    ):\n        if abs(scene_frame.ts - ts) &lt; 2e9 / fps:\n            frame_pixels = scene_frame.bgr\n        else:\n            frame_pixels = GrayFrame(scene_frame.width, scene_frame.height).bgr\n\n        if abs(gaze_datum.ts - ts) &lt; 2e9 / fps:\n            frame_pixels = cv2.circle(\n                frame_pixels,\n                (int(gaze_datum.x), int(gaze_datum.y)),\n                50,\n                (0, 0, 255),\n                10,\n            )\n\n        video_writer.write(frame_pixels)\n        cv2.imshow(\"Frame\", frame_pixels)\n        cv2.pollKey()\n\n    video_writer.release()\n\n\nif __name__ == \"__main__\":\n    make_overlaid_video(sys.argv[1], \"gaze-overlay-output-video.avi\", 24)\n</code></pre>"},{"location":"#imu","title":"Imu","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nimport pupil_labs.neon_recording as nr  # noqa: E402\n\nif len(sys.argv) &lt; 2:\n    print(\"Usage:\")\n    print(\"python imu.py path/to/recording/folder\")\n\n# Open a recording\nrecording = nr.open(sys.argv[1])\n\n# Sample the IMU data at 60Hz\nfps = 60\nz = recording.gaze[:10]\ntimestamps = np.arange(recording.imu.ts[0], recording.imu.ts[-1], 1e9 / fps)\nimu_data = recording.imu.sample(timestamps)\n\n# Use scipy to convert the quaternions to euler angles\nquaternions = np.array([s.quaternion_wxyz for s in imu_data])\nrotations = (\n    Rotation.from_quat(quaternions, scalar_first=True).as_euler(seq=\"yxz\", degrees=True)\n    % 360\n)\n\n# Combine the timestamps and eulers\nrotations_with_time = np.column_stack((timestamps, rotations))\ntimestamped_eulers = np.array(\n    [tuple(row) for row in rotations_with_time],\n    dtype=[\n        (\"ts\", np.int64),\n        (\"roll\", np.float64),\n        (\"pitch\", np.float64),\n        (\"yaw\", np.float64),\n    ],\n)\n\n# Display the angles\nframe_size = 512\ncolors = {\"pitch\": (0, 0, 255), \"yaw\": (0, 255, 0), \"roll\": (255, 0, 0)}\n\nfor row in timestamped_eulers:\n    # Create a blank image\n    frame = np.zeros((frame_size, frame_size, 3), dtype=np.uint8)\n\n    # Define the center and radius of the circles\n    center = [frame_size // 2] * 2\n    radius = frame.shape[0] // 3\n\n    # Calculate the end points for the angles\n    for field, color in colors.items():\n        pitch_end = (\n            int(center[0] + radius * np.cos(np.deg2rad(row[field]))),\n            int(center[1] - radius * np.sin(np.deg2rad(row[field]))),\n        )\n        cv2.line(frame, center, pitch_end, color, 2)\n\n        # Write the angle values on the image\n        cv2.putText(\n            frame,\n            f\"{field}: {row[field]:.2f}\",\n            (10, 30 + list(colors.keys()).index(field) * 30),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.7,\n            color,\n            2,\n        )\n\n    # Display the image\n    cv2.imshow(\"IMU Angles\", frame)\n    if cv2.waitKey(1000 // fps) == 27:\n        break\n\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"#worn","title":"Worn","text":"<pre><code>import sys\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs import neon_recording as nr  # noqa: E402\n\n\ndef write_text(image, text, x, y):\n    return cv2.putText(\n        image,\n        text,\n        (x, y),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        1,\n        (0, 0, 255),\n        2,\n        cv2.LINE_AA,\n    )\n\n\ndef make_overlaid_video(recording_dir, output_video_path, fps=30):\n    recording = nr.open(recording_dir)\n\n    video_writer = cv2.VideoWriter(\n        str(output_video_path),\n        cv2.VideoWriter_fourcc(*\"MJPG\"),\n        fps,\n        (recording.eye.width, recording.eye.height),\n    )\n\n    output_timestamps = np.arange(\n        recording.eye.ts[0], recording.eye.ts[-1], int(1e9 / fps)\n    )\n    eyes_and_worn = zip(\n        recording.eye.sample(output_timestamps),\n        recording.worn.sample(output_timestamps),\n    )\n\n    for frame, worn_record in tqdm(eyes_and_worn, total=len(output_timestamps)):\n        frame_pixels = frame.bgr\n\n        text_y = 40\n        if worn_record.worn:\n            frame_pixels = write_text(frame_pixels, \"Worn\", 0, text_y)\n\n        video_writer.write(frame_pixels)\n        cv2.imshow(\"Frame\", frame_pixels)\n        cv2.pollKey()\n\n    video_writer.release()\n\n\nif __name__ == \"__main__\":\n    make_overlaid_video(sys.argv[1], \"worn.mp4\")\n</code></pre>"},{"location":"contributing/","title":"Developer","text":""},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 Pupil Labs GmbH\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"modules/","title":"API reference","text":""},{"location":"modules/#pupil_labs.neon_recording","title":"neon_recording","text":"<p>Modules:</p> <ul> <li> <code>calib</code>           \u2013            <p>Camera calibration utils</p> </li> <li> <code>neon_recording</code>           \u2013            <p>Neon Recording</p> </li> <li> <code>stream</code>           \u2013            <p>Streams module</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>AudioStream</code>           \u2013            <p>Audio frames stream</p> </li> <li> <code>BlinkStream</code>           \u2013            <p>Blinks data</p> </li> <li> <code>EventStream</code>           \u2013            <p>Event annotations</p> </li> <li> <code>EyeStateStream</code>           \u2013            <p>Eye state data</p> </li> <li> <code>FixationStream</code>           \u2013            <p>Fixation data</p> </li> <li> <code>GazeStream</code>           \u2013            <p>Gaze data</p> </li> <li> <code>IMUStream</code>           \u2013            <p>Motion and orientation data</p> </li> <li> <code>NeonRecording</code>           \u2013            <p>Class to handle the Neon Recording data</p> </li> <li> <code>VideoStream</code>           \u2013            <p>Video frames from a camera</p> </li> <li> <code>WornStream</code>           \u2013            <p>Worn (headset on/off) data</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>open</code>             \u2013              <p>Load a NeonRecording from a path</p> </li> </ul>"},{"location":"modules/#pupil_labs.neon_recording.AudioStream","title":"AudioStream","text":"<pre><code>AudioStream(name: str, base_name: str, recording: NeonRecording)\n</code></pre> <p>               Bases: <code>BaseAVStream</code></p> <p>Audio frames stream</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>Array</code>)           \u2013            <p>Stream data as structured numpy array</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/av_stream/base_av_stream.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    base_name: str,\n    recording: \"NeonRecording\",\n):\n    self.name = name\n    self._base_name = base_name\n    self.recording = recording\n\n    log.debug(f\"NeonRecording: Loading video: {self._base_name}.\")\n\n    self.video_parts: list[plv.Reader[plv.VideoFrame]] = []\n    av_files = find_sorted_multipart_files(\n        self.recording._rec_dir, self._base_name, \".mp4\"\n    )\n    parts_ts = []\n    video_readers = []\n    for av_file, time_file in av_files:\n        if self.kind == \"video\":\n            part_ts = Array(time_file, dtype=TIMESTAMP_DTYPE)  # type: ignore\n            container_timestamps = (part_ts[\"ts\"] - recording.start_ts) / 1e9\n            reader = plv.Reader(str(av_file), self.kind, container_timestamps)\n            part_ts = part_ts[: len(reader)]\n        elif self.kind == \"audio\":\n            reader = plv.Reader(str(av_file), self.kind)  # type: ignore\n            part_ts = (\n                recording.start_ts + (reader.container_timestamps * 1e9)  # type: ignore\n            ).astype(TIMESTAMP_DTYPE)\n        else:\n            raise RuntimeError(f\"unknown av stream kind: {self.kind}\")\n\n        parts_ts.append(part_ts)\n        video_readers.append(reader)\n\n    parts_ts = np.concatenate(parts_ts)\n    idxs = np.empty(len(parts_ts), dtype=AV_INDEX_DTYPE)\n    idxs[AV_INDEX_FIELD_NAME] = np.arange(len(parts_ts))\n\n    data = join_struct_arrays(\n        [\n            parts_ts,  # type: ignore\n            idxs,\n        ],\n    )\n    self.av_reader = plv.MultiReader(video_readers)\n\n    BoundAVFrameClass = type(\n        f\"{self.name.capitalize()}Frame\",\n        (BaseAVStreamFrame, AVStreamProps),\n        {\"dtype\": data.dtype, \"multi_video_reader\": self.av_reader},\n    )\n    BoundAVFramesClass = type(\n        f\"{self.name.capitalize()}Frames\",\n        (Array, AVStreamProps),\n        {\n            \"record_class\": BoundAVFrameClass,\n            \"dtype\": data.dtype,\n            \"multi_video_reader\": self.av_reader,\n        },\n    )\n\n    super().__init__(name, recording, data.view(BoundAVFramesClass))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.AudioStream.data","title":"data  <code>property</code>","text":"<pre><code>data: Array\n</code></pre> <p>Stream data as structured numpy array</p>"},{"location":"modules/#pupil_labs.neon_recording.AudioStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.AudioStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.AudioStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream","title":"BlinkStream","text":"<pre><code>BlinkStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[BlinkArray, BlinkRecord]</code>, <code>BlinkProps</code></p> <p>Blinks data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>end_ts</code>           \u2013            <p>End timestamp of blink</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>start_ts</code>           \u2013            <p>Start timestamp of blink</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/blink_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading blink data\")\n    file_pairs = find_sorted_multipart_files(recording._rec_dir, \"blinks\")\n    data = load_multipart_data_time_pairs(\n        file_pairs,\n        np.dtype([\n            (\"start_timestamp_ns\", \"int64\"),\n            (\"end_timestamp_ns\", \"int64\"),\n        ]),\n    )\n    super().__init__(\"blink\", recording, data.view(BlinkArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream.end_ts","title":"end_ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_ts = fields[int64]('end_timestamp_ns')\n</code></pre> <p>End timestamp of blink</p>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream.start_ts","title":"start_ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_ts = fields[int64]('start_timestamp_ns')\n</code></pre> <p>Start timestamp of blink</p>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.BlinkStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.EventStream","title":"EventStream","text":"<pre><code>EventStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[EventArray, EventRecord]</code>, <code>EventProps</code></p> <p>Event annotations</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>by_name</code>           \u2013            <p>Return a dict of event_name =&gt; all ts</p> </li> <li> <code>event</code>           \u2013            <p>Event name</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/event_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading event data\")\n\n    events_file = recording._rec_dir / \"event.txt\"\n    time_file = events_file.with_suffix(\".time\")\n    file_pairs = []\n    if events_file.exists() and time_file.exists():\n        file_pairs = [(events_file, time_file)]\n    data = load_multipart_data_time_pairs(file_pairs, \"str\")\n    data.dtype.names = [\n        \"event\" if name == \"text\" else name for name in data.dtype.names\n    ]\n    super().__init__(\"event\", recording, data.view(EventArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.EventStream.by_name","title":"by_name  <code>cached</code> <code>property</code>","text":"<pre><code>by_name\n</code></pre> <p>Return a dict of event_name =&gt; all ts</p>"},{"location":"modules/#pupil_labs.neon_recording.EventStream.event","title":"event  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event = fields[float64]('event')\n</code></pre> <p>Event name</p>"},{"location":"modules/#pupil_labs.neon_recording.EventStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.EventStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.EventStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream","title":"EyeStateStream","text":"<pre><code>EyeStateStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[EyeStateArray, EyeStateRecord]</code>, <code>EyeStateProps</code></p> <p>Eye state data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>eyeball_center_left_xyz</code>           \u2013            <p>The xyz position in mm of the left eyeball relative to the scene camera</p> </li> <li> <code>eyeball_center_right_xyz</code>           \u2013            <p>The xyz position in mm of the right eyeball relative to the scene camera</p> </li> <li> <code>eyelid_angle</code>           \u2013            <p>Eyelid angle: (top_left, bottom_left, top_right, bottom_right)</p> </li> <li> <code>eyelid_aperture_left_right_mm</code>           \u2013            <p>Eyelid aperture in mm: (left, right)</p> </li> <li> <code>optical_axis_left_xyz</code>           \u2013            <p>A xyz vector in the forward direction of the left eye's optical axis</p> </li> <li> <code>optical_axis_right_xyz</code>           \u2013            <p>A xyz vector in the forward direction of the right eye's optical axis</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>pupil_diameter_left_mm</code>           \u2013            <p>Pupil diameter (in mm) for left eye</p> </li> <li> <code>pupil_diameter_left_right_mm</code>           \u2013            <p>Pupil diameter (in mm) for both eyes: (left, right)</p> </li> <li> <code>pupil_diameter_right_mm</code>           \u2013            <p>Pupil diameter (in mm) for right eye</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/eye_state_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading eye state data\")\n    file_pairs = find_sorted_multipart_files(recording._rec_dir, \"eye_state\")\n    data = load_multipart_data_time_pairs(\n        file_pairs,\n        dtype=np.dtype([\n            (\"pupil_diameter_left_mm\", \"float32\"),\n            (\"eyeball_center_left_x\", \"float32\"),\n            (\"eyeball_center_left_y\", \"float32\"),\n            (\"eyeball_center_left_z\", \"float32\"),\n            (\"optical_axis_left_x\", \"float32\"),\n            (\"optical_axis_left_y\", \"float32\"),\n            (\"optical_axis_left_z\", \"float32\"),\n            (\"pupil_diameter_right_mm\", \"float32\"),\n            (\"eyeball_center_right_x\", \"float32\"),\n            (\"eyeball_center_right_y\", \"float32\"),\n            (\"eyeball_center_right_z\", \"float32\"),\n            (\"optical_axis_right_x\", \"float32\"),\n            (\"optical_axis_right_y\", \"float32\"),\n            (\"optical_axis_right_z\", \"float32\"),\n        ]),\n    )\n    super().__init__(\"eye_state\", recording, data.view(EyeStateArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.eyeball_center_left_xyz","title":"eyeball_center_left_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_xyz = fields[float64](['eyeball_center_left_x', 'eyeball_center_left_y', 'eyeball_center_left_z'])\n</code></pre> <p>The xyz position in mm of the left eyeball relative to the scene camera</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.eyeball_center_right_xyz","title":"eyeball_center_right_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_xyz = fields[float64](['eyeball_center_right_x', 'eyeball_center_right_y', 'eyeball_center_right_z'])\n</code></pre> <p>The xyz position in mm of the right eyeball relative to the scene camera</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.eyelid_angle","title":"eyelid_angle  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eyelid_angle = fields[float64](['eyelid_angle_top_left', 'eyelid_angle_bottom_left', 'eyelid_angle_top_right', 'eyelid_angle_bottom_right'])\n</code></pre> <p>Eyelid angle: (top_left, bottom_left, top_right, bottom_right)</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.eyelid_aperture_left_right_mm","title":"eyelid_aperture_left_right_mm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eyelid_aperture_left_right_mm = fields[float64](['eyelid_aperture_left_mm', 'eyelid_aperture_right_mm'])\n</code></pre> <p>Eyelid aperture in mm: (left, right)</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.optical_axis_left_xyz","title":"optical_axis_left_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_xyz = fields[float64](['optical_axis_left_x', 'optical_axis_left_y', 'optical_axis_left_z'])\n</code></pre> <p>A xyz vector in the forward direction of the left eye's optical axis</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.optical_axis_right_xyz","title":"optical_axis_right_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_xyz = fields[float64](['optical_axis_right_x', 'optical_axis_right_y', 'optical_axis_right_z'])\n</code></pre> <p>A xyz vector in the forward direction of the right eye's optical axis</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.pupil_diameter_left_mm","title":"pupil_diameter_left_mm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left_mm = fields[float64](['pupil_diameter_left_mm'])\n</code></pre> <p>Pupil diameter (in mm) for left eye</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.pupil_diameter_left_right_mm","title":"pupil_diameter_left_right_mm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left_right_mm = fields[float64](['pupil_diameter_left_mm', 'pupil_diameter_right_mm'])\n</code></pre> <p>Pupil diameter (in mm) for both eyes: (left, right)</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.pupil_diameter_right_mm","title":"pupil_diameter_right_mm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right_mm = fields[float64](['pupil_diameter_right_mm'])\n</code></pre> <p>Pupil diameter (in mm) for right eye</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.EyeStateStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream","title":"FixationStream","text":"<pre><code>FixationStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[FixationArray, FixationRecord]</code>, <code>FixationProps</code></p> <p>Fixation data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>amplitude_angle_deg</code>           \u2013            <p>Amplitude angle (degrees)</p> </li> <li> <code>amplitude_pixels</code>           \u2013            <p>Amplitude (pixels)</p> </li> <li> <code>end_gaze_xy</code>           \u2013            <p>End gaze position in pixels</p> </li> <li> <code>end_ts</code>           \u2013            <p>Start timestamp of fixation</p> </li> <li> <code>event_type</code>           \u2013            <p>Fixation event kind (0 = saccade / 1 = fixation)</p> </li> <li> <code>max_velocity</code>           \u2013            <p>Max velocity of fixation (pixels/sec)</p> </li> <li> <code>mean_gaze_xy</code>           \u2013            <p>Mean gaze position in pixels</p> </li> <li> <code>mean_velocity</code>           \u2013            <p>Mean velocity of fixation (pixels/sec)</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>start_gaze_xy</code>           \u2013            <p>Start gaze position in pixels</p> </li> <li> <code>start_ts</code>           \u2013            <p>Start timestamp of fixation</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/fixation_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading fixation data\")\n    file_pairs = find_sorted_multipart_files(recording._rec_dir, \"fixations\")\n    data = load_multipart_data_time_pairs(\n        file_pairs,\n        np.dtype([\n            (\"event_type\", \"int32\"),\n            (\"start_timestamp_ns\", \"int64\"),\n            (\"end_timestamp_ns\", \"int64\"),\n            (\"start_gaze_x\", \"float32\"),\n            (\"start_gaze_y\", \"float32\"),\n            (\"end_gaze_x\", \"float32\"),\n            (\"end_gaze_y\", \"float32\"),\n            (\"mean_gaze_x\", \"float32\"),\n            (\"mean_gaze_y\", \"float32\"),\n            (\"amplitude_pixels\", \"float32\"),\n            (\"amplitude_angle_deg\", \"float32\"),\n            (\"mean_velocity\", \"float32\"),\n            (\"max_velocity\", \"float32\"),\n        ]),\n    )\n    super().__init__(\"fixation\", recording, data.view(FixationArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.amplitude_angle_deg","title":"amplitude_angle_deg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>amplitude_angle_deg = fields[float32]('amplitude_angle_deg')\n</code></pre> <p>Amplitude angle (degrees)</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.amplitude_pixels","title":"amplitude_pixels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>amplitude_pixels = fields[float32]('amplitude_pixels')\n</code></pre> <p>Amplitude (pixels)</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.end_gaze_xy","title":"end_gaze_xy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_gaze_xy = fields[float32](['end_gaze_x', 'end_gaze_y'])\n</code></pre> <p>End gaze position in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.end_ts","title":"end_ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_ts = fields[int64]('end_timestamp_ns')\n</code></pre> <p>Start timestamp of fixation</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.event_type","title":"event_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event_type = fields[int32]('event_type')\n</code></pre> <p>Fixation event kind (0 = saccade / 1 = fixation)</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.max_velocity","title":"max_velocity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_velocity = fields[float32]('max_velocity')\n</code></pre> <p>Max velocity of fixation (pixels/sec)</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.mean_gaze_xy","title":"mean_gaze_xy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mean_gaze_xy = fields[float32](['mean_gaze_x', 'mean_gaze_y'])\n</code></pre> <p>Mean gaze position in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.mean_velocity","title":"mean_velocity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mean_velocity = fields[float32]('mean_velocity')\n</code></pre> <p>Mean velocity of fixation (pixels/sec)</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.start_gaze_xy","title":"start_gaze_xy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_gaze_xy = fields[float32](['start_gaze_x', 'start_gaze_y'])\n</code></pre> <p>Start gaze position in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.start_ts","title":"start_ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_ts = fields[int64]('start_timestamp_ns')\n</code></pre> <p>Start timestamp of fixation</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.FixationStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream","title":"GazeStream","text":"<pre><code>GazeStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[GazeArray, GazeRecord]</code>, <code>GazeProps</code></p> <p>Gaze data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> <li> <code>x</code>           \u2013            <p>Gaze x coordinate in pixels</p> </li> <li> <code>xy</code>           \u2013            <p>Gaze xy coordinates in pixels</p> </li> <li> <code>y</code>           \u2013            <p>Gaze y coordinate in pixels</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/gaze_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading gaze data\")\n\n    gaze_200hz_file = recording._rec_dir / \"gaze_200hz.raw\"\n    time_200hz_file = recording._rec_dir / \"gaze_200hz.time\"\n\n    file_pairs = []\n    if gaze_200hz_file.exists() and time_200hz_file.exists():\n        log.debug(\"NeonRecording: Using 200Hz gaze data\")\n        file_pairs.append((gaze_200hz_file, time_200hz_file))\n    else:\n        log.debug(\"NeonRecording: Using realtime gaze data\")\n        file_pairs = find_sorted_multipart_files(recording._rec_dir, \"gaze\")\n\n    data = load_multipart_data_time_pairs(\n        file_pairs,\n        np.dtype([\n            (\"x\", \"float32\"),\n            (\"y\", \"float32\"),\n        ]),\n    )\n    super().__init__(\"gaze\", recording, data.view(GazeArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.x","title":"x  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>x = fields[float64]('x')\n</code></pre> <p>Gaze x coordinate in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.xy","title":"xy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>xy = fields[float64](['x', 'y'])\n</code></pre> <p>Gaze xy coordinates in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.y","title":"y  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>y = fields[float64]('y')\n</code></pre> <p>Gaze y coordinate in pixels</p>"},{"location":"modules/#pupil_labs.neon_recording.GazeStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream","title":"IMUStream","text":"<pre><code>IMUStream(recording)\n</code></pre> <p>               Bases: <code>Stream[ImuArray, ImuRecord]</code>, <code>ImuProps</code></p> <p>Motion and orientation data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>accel_xyz</code>           \u2013            <p>Acceleration data</p> </li> <li> <code>data</code>               (<code>Array</code>)           \u2013            <p>Stream data as structured numpy array</p> </li> <li> <code>gyro_xyz</code>           \u2013            <p>Gyroscope data</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>quaternion_wxyz</code>           \u2013            <p>Orientation as a quaternion</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/imu/imu_stream.py</code> <pre><code>def __init__(self, recording):\n    log.debug(\"NeonRecording: Loading IMU data\")\n\n    imu_file_pairs = find_sorted_multipart_files(recording._rec_dir, \"imu\")\n\n    if len(imu_file_pairs) &gt; 0:\n        imu_data = Array(  # type: ignore\n            [file for file, _ in imu_file_pairs],\n            fallback_dtype=np.dtype(IMUStream.FALLBACK_DTYPE),\n        )\n        imu_data.dtype.names = [  # type: ignore\n            TIMESTAMP_FIELD_NAME if name == \"timestamp_ns\" else name\n            for name in imu_data.dtype.names  # type: ignore\n        ]\n\n    else:\n        imu_file_pairs = find_sorted_multipart_files(recording._rec_dir, \"extimu\")\n        time_data = Array([file for _, file in imu_file_pairs], TIMESTAMP_DTYPE)  # type: ignore\n\n        records = []\n        for imu_file, _ in imu_file_pairs:\n            with imu_file.open(\"rb\") as raw_file:\n                raw_data = raw_file.read()\n                imu_packets = parse_neon_imu_raw_packets(raw_data)\n\n                records.extend([\n                    (\n                        packet.gyroData.x,\n                        packet.gyroData.y,\n                        packet.gyroData.z,\n                        packet.accelData.x,\n                        packet.accelData.y,\n                        packet.accelData.z,\n                        packet.rotVecData.w,\n                        packet.rotVecData.x,\n                        packet.rotVecData.y,\n                        packet.rotVecData.z,\n                    )\n                    for packet in imu_packets\n                ])\n\n        imu_data = np.array(records, dtype=IMUStream.FALLBACK_DTYPE)  # type: ignore\n        imu_data = join_struct_arrays([time_data, imu_data])\n\n    super().__init__(\"imu\", recording, imu_data.view(ImuArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.accel_xyz","title":"accel_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accel_xyz = fields[float64](['accel_x', 'accel_y', 'accel_z'])\n</code></pre> <p>Acceleration data</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.data","title":"data  <code>property</code>","text":"<pre><code>data: Array\n</code></pre> <p>Stream data as structured numpy array</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.gyro_xyz","title":"gyro_xyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gyro_xyz = fields[float64](['gyro_x', 'gyro_y', 'gyro_z'])\n</code></pre> <p>Gyroscope data</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.quaternion_wxyz","title":"quaternion_wxyz  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quaternion_wxyz = fields[float64](['quaternion_w', 'quaternion_x', 'quaternion_y', 'quaternion_z'])\n</code></pre> <p>Orientation as a quaternion</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.IMUStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording","title":"NeonRecording","text":"<pre><code>NeonRecording(rec_dir_in: Union[Path, str])\n</code></pre> <p>Class to handle the Neon Recording data</p> <p>Parameters:</p> <ul> <li> <code>rec_dir_in</code>               (<code>Union[Path, str]</code>)           \u2013            <p>Path to the recording directory.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the directory does not exist or is not valid.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>audio</code>               (<code>AudioStream</code>)           \u2013            <p>Audio from the scene video</p> </li> <li> <code>blinks</code>               (<code>BlinkStream</code>)           \u2013            <p>Blink data</p> </li> <li> <code>calibration</code>               (<code>Calibration | None</code>)           \u2013            <p>Device camera calibration data</p> </li> <li> <code>device_serial</code>               (<code>str | None</code>)           \u2013            <p>Device serial number</p> </li> <li> <code>duration</code>               (<code>int</code>)           \u2013            <p>Recording Duration (nanoseconds)</p> </li> <li> <code>events</code>               (<code>EventStream</code>)           \u2013            <p>Event annotations</p> </li> <li> <code>eye</code>               (<code>VideoStream</code>)           \u2013            <p>Frames of video from the eye cameras</p> </li> <li> <code>eye_state</code>               (<code>EyeStateStream</code>)           \u2013            <p>Eye state data</p> </li> <li> <code>fixations</code>               (<code>FixationStream</code>)           \u2013            <p>Fixations data</p> </li> <li> <code>gaze</code>               (<code>GazeStream</code>)           \u2013            <p>2D gaze data in scene-camera space</p> </li> <li> <code>id</code>               (<code>str | None</code>)           \u2013            <p>UUID of the recording</p> </li> <li> <code>imu</code>               (<code>IMUStream</code>)           \u2013            <p>Motion and orientation data</p> </li> <li> <code>info</code>               (<code>dict</code>)           \u2013            <p>Information loaded from info.json</p> </li> <li> <code>scene</code>               (<code>VideoStream</code>)           \u2013            <p>Frames of video from the scene camera</p> </li> <li> <code>start_ts</code>               (<code>int</code>)           \u2013            <p>Start timestamp (nanoseconds since 1970-01-01)</p> </li> <li> <code>stop_ts</code>               (<code>int</code>)           \u2013            <p>Stop timestamp (nanoseconds since 1970-01-01)</p> </li> <li> <code>wearer</code>               (<code>dict</code>)           \u2013            <p>Wearer information containing uuid and name</p> </li> <li> <code>worn</code>               (<code>WornStream</code>)           \u2013            <p>Worn (headset on/off) data</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/neon_recording.py</code> <pre><code>def __init__(self, rec_dir_in: Union[pathlib.Path, str]):\n    \"\"\"Initialize the NeonRecording object\n\n    Args:\n        rec_dir_in: Path to the recording directory.\n\n    Raises:\n        FileNotFoundError: If the directory does not exist or is not valid.\n\n    \"\"\"\n    self._rec_dir = pathlib.Path(rec_dir_in).resolve()\n    if not self._rec_dir.exists() or not self._rec_dir.is_dir():\n        raise FileNotFoundError(\n            f\"Directory not found or not valid: {self._rec_dir}\"\n        )\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.audio","title":"audio  <code>cached</code> <code>property</code>","text":"<pre><code>audio: AudioStream\n</code></pre> <p>Audio from the scene video</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.blinks","title":"blinks  <code>cached</code> <code>property</code>","text":"<pre><code>blinks: BlinkStream\n</code></pre> <p>Blink data</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.calibration","title":"calibration  <code>cached</code> <code>property</code>","text":"<pre><code>calibration: Calibration | None\n</code></pre> <p>Device camera calibration data</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.device_serial","title":"device_serial  <code>property</code>","text":"<pre><code>device_serial: str | None\n</code></pre> <p>Device serial number</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: int\n</code></pre> <p>Recording Duration (nanoseconds)</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.events","title":"events  <code>cached</code> <code>property</code>","text":"<pre><code>events: EventStream\n</code></pre> <p>Event annotations</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.eye","title":"eye  <code>cached</code> <code>property</code>","text":"<pre><code>eye: VideoStream\n</code></pre> <p>Frames of video from the eye cameras</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.eye_state","title":"eye_state  <code>cached</code> <code>property</code>","text":"<pre><code>eye_state: EyeStateStream\n</code></pre> <p>Eye state data</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.fixations","title":"fixations  <code>cached</code> <code>property</code>","text":"<pre><code>fixations: FixationStream\n</code></pre> <p>Fixations data</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.gaze","title":"gaze  <code>cached</code> <code>property</code>","text":"<pre><code>gaze: GazeStream\n</code></pre> <p>2D gaze data in scene-camera space</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>UUID of the recording</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.imu","title":"imu  <code>cached</code> <code>property</code>","text":"<pre><code>imu: IMUStream\n</code></pre> <p>Motion and orientation data</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.info","title":"info  <code>cached</code> <code>property</code>","text":"<pre><code>info: dict\n</code></pre> <p>Information loaded from info.json</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.scene","title":"scene  <code>cached</code> <code>property</code>","text":"<pre><code>scene: VideoStream\n</code></pre> <p>Frames of video from the scene camera</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.start_ts","title":"start_ts  <code>property</code>","text":"<pre><code>start_ts: int\n</code></pre> <p>Start timestamp (nanoseconds since 1970-01-01)</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.stop_ts","title":"stop_ts  <code>property</code>","text":"<pre><code>stop_ts: int\n</code></pre> <p>Stop timestamp (nanoseconds since 1970-01-01)</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.wearer","title":"wearer  <code>cached</code> <code>property</code>","text":"<pre><code>wearer: dict\n</code></pre> <p>Wearer information containing uuid and name</p>"},{"location":"modules/#pupil_labs.neon_recording.NeonRecording.worn","title":"worn  <code>cached</code> <code>property</code>","text":"<pre><code>worn: WornStream\n</code></pre> <p>Worn (headset on/off) data</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream","title":"VideoStream","text":"<pre><code>VideoStream(name: str, base_name: str, recording: NeonRecording)\n</code></pre> <p>               Bases: <code>BaseAVStream</code></p> <p>Video frames from a camera</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>Array</code>)           \u2013            <p>Stream data as structured numpy array</p> </li> <li> <code>height</code>               (<code>int | None</code>)           \u2013            <p>Height of image in stream</p> </li> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> <li> <code>width</code>               (<code>int | None</code>)           \u2013            <p>Width of image in stream</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/av_stream/base_av_stream.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    base_name: str,\n    recording: \"NeonRecording\",\n):\n    self.name = name\n    self._base_name = base_name\n    self.recording = recording\n\n    log.debug(f\"NeonRecording: Loading video: {self._base_name}.\")\n\n    self.video_parts: list[plv.Reader[plv.VideoFrame]] = []\n    av_files = find_sorted_multipart_files(\n        self.recording._rec_dir, self._base_name, \".mp4\"\n    )\n    parts_ts = []\n    video_readers = []\n    for av_file, time_file in av_files:\n        if self.kind == \"video\":\n            part_ts = Array(time_file, dtype=TIMESTAMP_DTYPE)  # type: ignore\n            container_timestamps = (part_ts[\"ts\"] - recording.start_ts) / 1e9\n            reader = plv.Reader(str(av_file), self.kind, container_timestamps)\n            part_ts = part_ts[: len(reader)]\n        elif self.kind == \"audio\":\n            reader = plv.Reader(str(av_file), self.kind)  # type: ignore\n            part_ts = (\n                recording.start_ts + (reader.container_timestamps * 1e9)  # type: ignore\n            ).astype(TIMESTAMP_DTYPE)\n        else:\n            raise RuntimeError(f\"unknown av stream kind: {self.kind}\")\n\n        parts_ts.append(part_ts)\n        video_readers.append(reader)\n\n    parts_ts = np.concatenate(parts_ts)\n    idxs = np.empty(len(parts_ts), dtype=AV_INDEX_DTYPE)\n    idxs[AV_INDEX_FIELD_NAME] = np.arange(len(parts_ts))\n\n    data = join_struct_arrays(\n        [\n            parts_ts,  # type: ignore\n            idxs,\n        ],\n    )\n    self.av_reader = plv.MultiReader(video_readers)\n\n    BoundAVFrameClass = type(\n        f\"{self.name.capitalize()}Frame\",\n        (BaseAVStreamFrame, AVStreamProps),\n        {\"dtype\": data.dtype, \"multi_video_reader\": self.av_reader},\n    )\n    BoundAVFramesClass = type(\n        f\"{self.name.capitalize()}Frames\",\n        (Array, AVStreamProps),\n        {\n            \"record_class\": BoundAVFrameClass,\n            \"dtype\": data.dtype,\n            \"multi_video_reader\": self.av_reader,\n        },\n    )\n\n    super().__init__(name, recording, data.view(BoundAVFramesClass))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.data","title":"data  <code>property</code>","text":"<pre><code>data: Array\n</code></pre> <p>Stream data as structured numpy array</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.height","title":"height  <code>property</code>","text":"<pre><code>height: int | None\n</code></pre> <p>Height of image in stream</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.width","title":"width  <code>property</code>","text":"<pre><code>width: int | None\n</code></pre> <p>Width of image in stream</p>"},{"location":"modules/#pupil_labs.neon_recording.VideoStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.WornStream","title":"WornStream","text":"<pre><code>WornStream(recording: NeonRecording)\n</code></pre> <p>               Bases: <code>Stream[WornArray, WornRecord]</code>, <code>WornProps</code></p> <p>Worn (headset on/off) data</p> <p>Methods:</p> <ul> <li> <code>interpolate</code>             \u2013              <p>Interpolated stream data for <code>sorted_ts</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>pd</code>           \u2013            <p>Stream data as a pandas DataFrame</p> </li> <li> <code>ts</code>               (<code>NDArray[int64]</code>)           \u2013            <p>The moment these data were recorded</p> </li> <li> <code>worn</code>           \u2013            <p>Worn</p> </li> </ul> Source code in <code>src/pupil_labs/neon_recording/stream/worn_stream.py</code> <pre><code>def __init__(self, recording: \"NeonRecording\"):\n    log.debug(\"NeonRecording: Loading worn data\")\n\n    worn_200hz_file = recording._rec_dir / \"worn_200hz.raw\"\n    time_200hz_file = recording._rec_dir / \"gaze_200hz.time\"\n\n    file_pairs = []\n    if worn_200hz_file.exists() and time_200hz_file.exists():\n        log.debug(\"NeonRecording: Using 200Hz worn data\")\n        file_pairs.append((worn_200hz_file, time_200hz_file))\n    else:\n        log.debug(\"NeonRecording: Using realtime worn data\")\n        file_pairs = find_sorted_multipart_files(recording._rec_dir, \"worn\")\n\n    data = load_multipart_data_time_pairs(file_pairs, np.dtype([(\"worn\", \"u1\")]))\n    super().__init__(\"worn\", recording, data.view(WornArray))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.WornStream.pd","title":"pd  <code>property</code>","text":"<pre><code>pd\n</code></pre> <p>Stream data as a pandas DataFrame</p>"},{"location":"modules/#pupil_labs.neon_recording.WornStream.ts","title":"ts  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts: NDArray[int64] = fields[int64](TIMESTAMP_FIELD_NAME)\n</code></pre> <p>The moment these data were recorded</p>"},{"location":"modules/#pupil_labs.neon_recording.WornStream.worn","title":"worn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>worn = fields[float64]('worn')\n</code></pre> <p>Worn</p>"},{"location":"modules/#pupil_labs.neon_recording.WornStream.interpolate","title":"interpolate","text":"<pre><code>interpolate(sorted_ts: NDArray[int64]) -&gt; ArrayType\n</code></pre> <p>Interpolated stream data for <code>sorted_ts</code></p> Source code in <code>src/pupil_labs/neon_recording/stream/stream.py</code> <pre><code>def interpolate(self, sorted_ts: npt.NDArray[np.int64]) -&gt; ArrayType:\n    \"\"\"Interpolated stream data for `sorted_ts`\"\"\"\n    assert self.data.dtype is not None\n\n    sorted_ts = np.array(sorted_ts)\n    interpolated_dtype = np.dtype([\n        (k, np.int64 if k == TIMESTAMP_FIELD_NAME else np.float64)\n        for k in self.data.dtype.names or []\n        if issubclass(self.data.dtype[k].type, (np.floating, np.integer))\n    ])\n\n    result = np.zeros(len(sorted_ts), interpolated_dtype)\n    result[TIMESTAMP_FIELD_NAME] = sorted_ts\n    for key in interpolated_dtype.names or []:\n        if key == TIMESTAMP_FIELD_NAME:\n            continue\n        value = self.data[key].astype(np.float64)\n        result[key] = np.interp(\n            sorted_ts,\n            self.ts,\n            value,\n            left=np.nan,\n            right=np.nan,\n        )\n    return cast(ArrayType, result.view(self.data.__class__))\n</code></pre>"},{"location":"modules/#pupil_labs.neon_recording.open","title":"open","text":"<pre><code>open(rec_dir_in: Union[Path, str]) -&gt; NeonRecording\n</code></pre> <p>Load a NeonRecording from a path</p> Source code in <code>src/pupil_labs/neon_recording/neon_recording.py</code> <pre><code>def open(rec_dir_in: Union[pathlib.Path, str]) -&gt; NeonRecording:  # noqa: A001\n    \"\"\"Load a NeonRecording from a path\"\"\"\n    return NeonRecording(rec_dir_in)\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}